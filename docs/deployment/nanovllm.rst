========================
Nano-vLLM
========================

.. warning::
    This guide is under construction.

NanoVLLM-VoxCPM is a deployment solution for VoxCPM using NanoVLLM. It is a lightweight vLLM framework that is modified for VoxCPM architecture.

Features
--------

* High throughput on CUDA devices (RTX 4090, H100, A100, etc.)
* Support VoxCPM-1.0 and VoxCPM-1.5
* Support streaming inference
* Support batch inference
* Support multi-GPU inference


Installation
------------

See the `docs <https://github.com/a710128/nanovllm-voxcpm/blob/main/README.md>`_ here.
